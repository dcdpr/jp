---
source: crates/jp_test/src/mock.rs
expression: v
---
[
  {
    "type": "config_delta",
    "timestamp": "2025-12-13 07:06:57.649955",
    "delta": {
      "inherit": false,
      "config_load_paths": [],
      "extends": [],
      "assistant": {
        "tool_choice": "auto",
        "model": {
          "id": {
            "provider": "google",
            "name": "test"
          },
          "parameters": {
            "reasoning": "off",
            "stop_words": [],
            "other": {}
          }
        }
      },
      "conversation": {
        "title": {
          "generate": {
            "auto": false
          }
        },
        "tools": {
          "*": {
            "run": "ask",
            "result": "unattended",
            "style": {
              "inline_results": {
                "truncate": {
                  "lines": 10
                }
              },
              "results_file_link": "full"
            }
          }
        }
      },
      "style": {
        "code": {
          "theme": "",
          "color": false,
          "line_numbers": false,
          "file_link": "osc8",
          "copy_link": "osc8"
        },
        "reasoning": {
          "display": "full"
        },
        "tool_call": {
          "show": false
        },
        "typewriter": {
          "text_delay": {
            "secs": 0,
            "nanos": 0
          },
          "code_delay": {
            "secs": 0,
            "nanos": 0
          }
        }
      },
      "editor": {
        "envs": []
      },
      "template": {
        "values": {}
      },
      "providers": {
        "llm": {
          "anthropic": {
            "api_key_env": "",
            "base_url": "",
            "chain_on_max_tokens": false,
            "beta_headers": []
          },
          "deepseek": {
            "api_key_env": ""
          },
          "google": {
            "api_key_env": "",
            "base_url": ""
          },
          "llamacpp": {
            "base_url": ""
          },
          "ollama": {
            "base_url": ""
          },
          "openai": {
            "api_key_env": "",
            "base_url": "",
            "base_url_env": ""
          },
          "openrouter": {
            "api_key_env": "",
            "app_name": "",
            "base_url": ""
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Test message"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "Okay, I received your test message! It's working. How can I help you today?"
  },
  {
    "type": "config_delta",
    "timestamp": "2025-12-13 07:06:55.485624",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": {
              "effort": "low",
              "exclude": false
            }
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Repeat my previous message"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "reasoning": "**Reflecting on Repetition**\n\nI'm pondering the potential for unintended repetition. The current focus is on maintaining diversity in output while effectively echoing past communications. I'm investigating methods to achieve this nuance, so that the response remains useful and engaging, rather than rote or uninspired.\n\n\n"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "Your previous message was: \"Test message\""
  },
  {
    "type": "config_delta",
    "timestamp": "2025-12-13 07:06:55.485703",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": "off"
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Testing tool call"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "tool_call_request",
    "id": "",
    "name": "run_me",
    "arguments": {
      "bar": "Zm9v"
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "tool_call_response",
    "id": "",
    "content": "VGhlIHNlY3JldCBjb2RlIGlzOiA0Mg==",
    "is_error": false
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "The secret code is: 42"
  },
  {
    "type": "config_delta",
    "timestamp": "2025-12-13 07:06:55.485836",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": {
              "effort": "low",
              "exclude": false
            }
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "What was the result of the previous tool call?"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "reasoning": "**Reviewing Previous Actions**\n\nI'm currently trying to recall the results of my last tool call, a crucial step based on the user's latest query. I'm digging into my recent history to pinpoint the output of \"Testing tool call.\" The user's prompt directly suggests a need for this previous result, so I'm focusing my search there.\n\n\n**Accessing Past Data**\n\nI'm now fully immersed in retrieving the necessary information. It's imperative that I access and present the details from the most recent tool execution. Specifically, I'm pulling the results stemming from the \"Testing tool call\". My memory indicates I ran a `print(default_api.run_me(bar = \"foo\"))` and the output returned was `{\"run_me_response\": {\"content\": \"The secret code is: 42\"}}`. My response will be streamlined and directly address the user's needs.\n\n\n"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "The previous tool call returned: `{'run_me_response': {'content': 'The secret code is: 42'}}`"
  }
]
