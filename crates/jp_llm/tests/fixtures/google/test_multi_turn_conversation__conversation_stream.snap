---
source: crates/jp_test/src/mock.rs
expression: v
---
[
  {
    "type": "config_delta",
    "timestamp": "2020-01-01 00:00:00.0",
    "delta": {
      "inherit": false,
      "config_load_paths": [],
      "extends": [],
      "assistant": {
        "tool_choice": "auto",
        "model": {
          "id": {
            "provider": "google",
            "name": "test"
          },
          "parameters": {
            "reasoning": "off",
            "stop_words": [],
            "other": {}
          }
        }
      },
      "conversation": {
        "title": {
          "generate": {
            "auto": false
          }
        },
        "tools": {
          "*": {
            "run": "ask",
            "result": "unattended",
            "style": {
              "inline_results": {
                "truncate": {
                  "lines": 10
                }
              },
              "results_file_link": "full",
              "parameters": "json"
            }
          }
        }
      },
      "style": {
        "code": {
          "theme": "",
          "color": false,
          "line_numbers": false,
          "file_link": "osc8",
          "copy_link": "osc8"
        },
        "reasoning": {
          "display": "full"
        },
        "tool_call": {
          "show": false
        },
        "typewriter": {
          "text_delay": {
            "secs": 0,
            "nanos": 0
          },
          "code_delay": {
            "secs": 0,
            "nanos": 0
          }
        }
      },
      "editor": {
        "envs": []
      },
      "template": {
        "values": {}
      },
      "providers": {
        "llm": {
          "anthropic": {
            "api_key_env": "",
            "base_url": "",
            "chain_on_max_tokens": false,
            "beta_headers": []
          },
          "deepseek": {
            "api_key_env": ""
          },
          "google": {
            "api_key_env": "",
            "base_url": ""
          },
          "llamacpp": {
            "base_url": ""
          },
          "ollama": {
            "base_url": ""
          },
          "openai": {
            "api_key_env": "",
            "base_url": "",
            "base_url_env": ""
          },
          "openrouter": {
            "api_key_env": "",
            "app_name": "",
            "base_url": ""
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Test message"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "Hello! I am a large language model, trained by Google. How can I help you today?"
  },
  {
    "type": "config_delta",
    "timestamp": "2020-01-01 00:00:00.0",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": {
              "effort": "low",
              "exclude": false
            }
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Repeat my previous message"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "reasoning": "**Analyzing Task Requirements**\n\nI've been examining the user's instructions and am focused on understanding the core elements: recognizing and repeating the preceding message. I am now working to ensure that future responses precisely follow the constraints regarding format and word count. My objective is to create updates that accurately reflect any progress.\n\n\n"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "Test message"
  },
  {
    "type": "config_delta",
    "timestamp": "2020-01-01 00:00:00.0",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": "off"
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "Please run the tool, providing whatever arguments you want."
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "tool_call_request",
    "id": "run_me_0",
    "name": "run_me",
    "arguments": {
      "bar": "Zm9v"
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "tool_call_response",
    "id": "run_me_0",
    "content": "VGhlIHNlY3JldCBjb2RlIGlzOiA0Mg==",
    "is_error": false
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "The secret code is: 42"
  },
  {
    "type": "config_delta",
    "timestamp": "2020-01-01 00:00:00.0",
    "delta": {
      "assistant": {
        "model": {
          "parameters": {
            "reasoning": {
              "effort": "low",
              "exclude": false
            }
          }
        }
      }
    }
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_request",
    "content": "What was the result of the previous tool call?"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "reasoning": "**Refining Understanding**\n\nI'm now focusing on directly mapping the user's query about the \"previous tool call result\" to the output from the most recent tool execution. Specifically, I'm identifying that the prior turn used `default_api.run_me` and returned `{\"run_me_response\": {\"content\": \"The secret code\"}}`. This clarifies my approach.\n\n\n**Clarifying Tool Usage**\n\nI've zeroed in on the specifics of the tool interaction. It's clear the previous turn executed `default_api.run_me`, with a returned value: `{\"run_me_response\": {\"content\": \"The secret code is: 42\"}}`. My response now isolates the essential bit; extracting the value associated with the `content` key within the `run_me_response`. This simplifies the process.\n\n\n"
  },
  {
    "timestamp": "2020-01-01 00:00:00.0",
    "type": "chat_response",
    "message": "The result of the previous tool call was: \"The secret code is: 42\"."
  }
]
