when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |-
    data: {"candidates": [{"content": {"parts": [{"text": "Okay, I received"}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 3,"candidatesTokenCount": 4,"totalTokenCount": 7,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 3}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "jRA9aevkI93insEPj9me6Qc"}

    data: {"candidates": [{"content": {"parts": [{"text": " your test message! It's working. How can I help you today?"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 3,"candidatesTokenCount": 20,"totalTokenCount": 23,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 3}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "jRA9aevkI93insEPj9me6Qc"}
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Okay, I received your test message! It's working. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": true,
          "thinkingBudget": 12800,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |-
    data: {"candidates": [{"content": {"parts": [{"text": "**Reflecting on Repetition**\n\nI'm pondering the potential for unintended repetition. The current focus is on maintaining diversity in output while effectively echoing past communications. I'm investigating methods to achieve this nuance, so that the response remains useful and engaging, rather than rote or uninspired.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 29,"totalTokenCount": 50,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 29}],"thoughtsTokenCount": 21},"modelVersion": "gemini-2.5-flash-lite","responseId": "jRA9aaTCMOKmkdUP7_SdkAc"}

    data: {"candidates": [{"content": {"parts": [{"text": "Your previous message was: \"Test message\""}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 29,"candidatesTokenCount": 8,"totalTokenCount": 58,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 29}],"thoughtsTokenCount": 21},"modelVersion": "gemini-2.5-flash-lite","responseId": "jRA9aaTCMOKmkdUP7_SdkAc"}
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Okay, I received your test message! It's working. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Reflecting on Repetition**\\n\\nI'm pondering the potential for unintended repetition. The current focus is on maintaining diversity in output while effectively echoing past communications. I'm investigating methods to achieve this nuance, so that the response remains useful and engaging, rather than rote or uninspired.\\n\\n\\n"
            },
            {
              "text": "Your previous message was: \"Test message\""
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Testing tool call"
            }
          ],
          "role": "user"
        }
      ],
      "tools": [
        {
          "functionDeclarations": [
            {
              "name": "run_me",
              "description": "",
              "parameters": null,
              "parametersJsonSchema": {
                "type": "object",
                "properties": {
                  "foo": {
                    "default": "foo",
                    "type": "string"
                  },
                  "bar": {
                    "anyOf": [
                      {
                        "type": "string",
                        "enum": [
                          "foo"
                        ]
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "enum": [
                          [
                            "foo",
                            "bar"
                          ]
                        ]
                      }
                    ]
                  }
                },
                "additionalProperties": false,
                "required": [
                  "bar"
                ]
              },
              "response": null
            }
          ]
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "ANY",
          "allowedFunctionNames": [
            "run_me"
          ]
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: "data: {\"candidates\": [{\"content\": {\"parts\": [{\"functionCall\": {\"name\": \"run_me\",\"args\": {\"bar\": \"foo\"}}}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0,\"finishMessage\": \"Model generated function call(s).\"}],\"usageMetadata\": {\"promptTokenCount\": 158,\"candidatesTokenCount\": 15,\"totalTokenCount\": 173,\"promptTokensDetails\": [{\"modality\": \"TEXT\",\"tokenCount\": 158}]},\"modelVersion\": \"gemini-2.5-flash-lite\",\"responseId\": \"jxA9aaCEAvTMkdUPxvOewAc\"}"
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Okay, I received your test message! It's working. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Reflecting on Repetition**\\n\\nI'm pondering the potential for unintended repetition. The current focus is on maintaining diversity in output while effectively echoing past communications. I'm investigating methods to achieve this nuance, so that the response remains useful and engaging, rather than rote or uninspired.\\n\\n\\n"
            },
            {
              "text": "Your previous message was: \"Test message\""
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Testing tool call"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "functionCall": {
                "id": "",
                "name": "run_me",
                "args": {
                  "bar": "foo"
                }
              }
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "functionResponse": {
                "id": "",
                "name": "run_me",
                "response": {
                  "content": "The secret code is: 42"
                }
              }
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |-
    data: {"candidates": [{"content": {"parts": [{"text": "The"}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 140,"candidatesTokenCount": 1,"totalTokenCount": 141,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 140}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "jxA9ac2mFp-kkdUPopbSwQc"}

    data: {"candidates": [{"content": {"parts": [{"text": " secret code is: 42"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 140,"candidatesTokenCount": 8,"totalTokenCount": 148,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 140}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "jxA9ac2mFp-kkdUPopbSwQc"}
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Okay, I received your test message! It's working. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Reflecting on Repetition**\\n\\nI'm pondering the potential for unintended repetition. The current focus is on maintaining diversity in output while effectively echoing past communications. I'm investigating methods to achieve this nuance, so that the response remains useful and engaging, rather than rote or uninspired.\\n\\n\\n"
            },
            {
              "text": "Your previous message was: \"Test message\""
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Testing tool call"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "functionCall": {
                "id": "",
                "name": "run_me",
                "args": {
                  "bar": "foo"
                }
              }
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "functionResponse": {
                "id": "",
                "name": "run_me",
                "response": {
                  "content": "The secret code is: 42"
                }
              }
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "The secret code is: 42"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "What was the result of the previous tool call?"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": true,
          "thinkingBudget": 12800,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |-
    data: {"candidates": [{"content": {"parts": [{"text": "**Reviewing Previous Actions**\n\nI'm currently trying to recall the results of my last tool call, a crucial step based on the user's latest query. I'm digging into my recent history to pinpoint the output of \"Testing tool call.\" The user's prompt directly suggests a need for this previous result, so I'm focusing my search there.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 160,"totalTokenCount": 231,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 160}],"thoughtsTokenCount": 71},"modelVersion": "gemini-2.5-flash-lite","responseId": "jxA9aYjJMdHi7M8PwKWxsAc"}

    data: {"candidates": [{"content": {"parts": [{"text": "**Accessing Past Data**\n\nI'm now fully immersed in retrieving the necessary information. It's imperative that I access and present the details from the most recent tool execution. Specifically, I'm pulling the results stemming from the \"Testing tool call\". My memory indicates I ran a `print(default_api.run_me(bar = \"foo\"))` and the output returned was `{\"run_me_response\": {\"content\": \"The secret code is: 42\"}}`. My response will be streamlined and directly address the user's needs.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 160,"totalTokenCount": 368,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 160}],"thoughtsTokenCount": 208},"modelVersion": "gemini-2.5-flash-lite","responseId": "jxA9aYjJMdHi7M8PwKWxsAc"}

    data: {"candidates": [{"content": {"parts": [{"text": "The previous tool call returned: `{'run_me_response': {'content': 'The secret code is: 42'}}`"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 160,"candidatesTokenCount": 26,"totalTokenCount": 394,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 160}],"thoughtsTokenCount": 208},"modelVersion": "gemini-2.5-flash-lite","responseId": "jxA9aYjJMdHi7M8PwKWxsAc"}
