when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |+
    data: {"candidates": [{"content": {"parts": [{"text": "Hello"}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 3,"candidatesTokenCount": 1,"totalTokenCount": 4,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 3}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "Wh1FaYCxEqqYvdIPy7X9UA"}
    
    data: {"candidates": [{"content": {"parts": [{"text": "! I am a large language model, trained by Google. How can I help you today?"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 3,"candidatesTokenCount": 20,"totalTokenCount": 23,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 3}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "Wh1FaYCxEqqYvdIPy7X9UA"}
    
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Hello! I am a large language model, trained by Google. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": true,
          "thinkingBudget": 12800,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |+
    data: {"candidates": [{"content": {"parts": [{"text": "**Analyzing Task Requirements**\n\nI've been examining the user's instructions and am focused on understanding the core elements: recognizing and repeating the preceding message. I am now working to ensure that future responses precisely follow the constraints regarding format and word count. My objective is to create updates that accurately reflect any progress.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 29,"totalTokenCount": 58,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 29}],"thoughtsTokenCount": 29},"modelVersion": "gemini-2.5-flash-lite","responseId": "Wh1FaaP4L5fWxN8P9-mp8Ak"}
    
    data: {"candidates": [{"content": {"parts": [{"text": "Test message"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 29,"candidatesTokenCount": 1,"totalTokenCount": 59,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 29}],"thoughtsTokenCount": 29},"modelVersion": "gemini-2.5-flash-lite","responseId": "Wh1FaaP4L5fWxN8P9-mp8Ak"}
    
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Hello! I am a large language model, trained by Google. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Analyzing Task Requirements**\n\nI've been examining the user's instructions and am focused on understanding the core elements: recognizing and repeating the preceding message. I am now working to ensure that future responses precisely follow the constraints regarding format and word count. My objective is to create updates that accurately reflect any progress.\n\n\n"
            },
            {
              "text": "Test message"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Please run the tool, providing whatever arguments you want."
            }
          ],
          "role": "user"
        }
      ],
      "tools": [
        {
          "functionDeclarations": [
            {
              "name": "run_me",
              "description": "",
              "parameters": null,
              "parametersJsonSchema": {
                "type": "object",
                "properties": {
                  "foo": {
                    "type": "string",
                    "default": "foo"
                  },
                  "bar": {
                    "type": [
                      "string",
                      "array"
                    ],
                    "enum": [
                      "foo",
                      [
                        "foo",
                        "bar"
                      ]
                    ],
                    "items": {
                      "type": "string"
                    }
                  }
                },
                "additionalProperties": false,
                "required": [
                  "bar"
                ]
              },
              "response": null
            }
          ]
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "ANY",
          "allowedFunctionNames": [
            "run_me"
          ]
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |+
    data: {"candidates": [{"content": {"parts": [{"functionCall": {"name": "run_me","args": {"bar": "foo"}}}],"role": "model"},"finishReason": "STOP","index": 0,"finishMessage": "Model generated function call(s)."}],"usageMetadata": {"promptTokenCount": 163,"candidatesTokenCount": 15,"totalTokenCount": 178,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 163}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1Faaz7BJPHxN8PxKrGsAw"}
    
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Hello! I am a large language model, trained by Google. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Analyzing Task Requirements**\n\nI've been examining the user's instructions and am focused on understanding the core elements: recognizing and repeating the preceding message. I am now working to ensure that future responses precisely follow the constraints regarding format and word count. My objective is to create updates that accurately reflect any progress.\n\n\n"
            },
            {
              "text": "Test message"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Please run the tool, providing whatever arguments you want."
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "functionCall": {
                "id": "run_me_0",
                "name": "run_me",
                "args": {
                  "bar": "foo"
                }
              },
              "thoughtSignature": "skip_thought_signature_validator"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "functionResponse": {
                "id": "run_me_0",
                "name": "run_me",
                "response": {
                  "content": "The secret code is: 42"
                }
              }
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": false,
          "thinkingBudget": null,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |+
    data: {"candidates": [{"content": {"parts": [{"text": "The secret"}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 145,"candidatesTokenCount": 2,"totalTokenCount": 147,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 145}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1FaayrHMS_xN8Pkeu1gQI"}
    
    data: {"candidates": [{"content": {"parts": [{"text": " code is: 42"}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 145,"candidatesTokenCount": 8,"totalTokenCount": 153,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 145}]},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1FaayrHMS_xN8Pkeu1gQI"}
    
---
when:
  path: "/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent"
  method: POST
  query_param:
    - name: alt
      value: sse
  json_body_str: >-
    {
      "contents": [
        {
          "parts": [
            {
              "text": "Test message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "Hello! I am a large language model, trained by Google. How can I help you today?"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Repeat my previous message"
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "thought": true,
              "text": "**Analyzing Task Requirements**\n\nI've been examining the user's instructions and am focused on understanding the core elements: recognizing and repeating the preceding message. I am now working to ensure that future responses precisely follow the constraints regarding format and word count. My objective is to create updates that accurately reflect any progress.\n\n\n"
            },
            {
              "text": "Test message"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "Please run the tool, providing whatever arguments you want."
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "functionCall": {
                "id": "run_me_0",
                "name": "run_me",
                "args": {
                  "bar": "foo"
                }
              },
              "thoughtSignature": "skip_thought_signature_validator"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "functionResponse": {
                "id": "run_me_0",
                "name": "run_me",
                "response": {
                  "content": "The secret code is: 42"
                }
              }
            }
          ],
          "role": "user"
        },
        {
          "parts": [
            {
              "text": "The secret code is: 42"
            }
          ],
          "role": "model"
        },
        {
          "parts": [
            {
              "text": "What was the result of the previous tool call?"
            }
          ],
          "role": "user"
        }
      ],
      "toolConfig": {
        "functionCallingConfig": {
          "mode": "NONE",
          "allowedFunctionNames": []
        }
      },
      "generationConfig": {
        "maxOutputTokens": 64000,
        "thinkingConfig": {
          "includeThoughts": true,
          "thinkingBudget": 12800,
          "thinkingLevel": null
        }
      }
    }
then:
  status: 200
  header:
    - name: content-type
      value: text/event-stream
  body: |+
    data: {"candidates": [{"content": {"parts": [{"text": "**Refining Understanding**\n\nI'm now focusing on directly mapping the user's query about the \"previous tool call result\" to the output from the most recent tool execution. Specifically, I'm identifying that the prior turn used `default_api.run_me` and returned `{\"run_me_response\": {\"content\": \"The secret code\"}}`. This clarifies my approach.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 165,"totalTokenCount": 236,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 165}],"thoughtsTokenCount": 71},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1Faav9NOqivdIPmfvc0QE"}
    
    data: {"candidates": [{"content": {"parts": [{"text": "**Clarifying Tool Usage**\n\nI've zeroed in on the specifics of the tool interaction. It's clear the previous turn executed `default_api.run_me`, with a returned value: `{\"run_me_response\": {\"content\": \"The secret code is: 42\"}}`. My response now isolates the essential bit; extracting the value associated with the `content` key within the `run_me_response`. This simplifies the process.\n\n\n","thought": true}],"role": "model"},"index": 0}],"usageMetadata": {"promptTokenCount": 165,"totalTokenCount": 298,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 165}],"thoughtsTokenCount": 133},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1Faav9NOqivdIPmfvc0QE"}
    
    data: {"candidates": [{"content": {"parts": [{"text": "The result of the previous tool call was: \"The secret code is: 42\"."}],"role": "model"},"finishReason": "STOP","index": 0}],"usageMetadata": {"promptTokenCount": 165,"candidatesTokenCount": 17,"totalTokenCount": 315,"promptTokensDetails": [{"modality": "TEXT","tokenCount": 165}],"thoughtsTokenCount": 133},"modelVersion": "gemini-2.5-flash-lite","responseId": "XR1Faav9NOqivdIPmfvc0QE"}
    
